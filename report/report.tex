\documentclass[justified]{article}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage[margin=1.1in]{geometry}

\usepackage{microtype}
\usepackage{mathpazo}

\begin{document}
  \title{Signal Processing Methods for Noise-Resistant CNNs}
  \author{Jonathan Lee}
  \date{\today}
  \maketitle

  \section{Problem Statement and Background}
  % Motivation, related work, summary of contribution, results

  \section{Approach}
  % Assumptions, definitions, objective

  \begin{enumerate}
  \item Augment the dataset with rotated, shifted, scaled (input perturbations)
  \item Add noise (AWGN, one-shot, patches), train denoising network
  \item ResNet-like architecture
  %+ Additive white Gaussian noise layers?
  % denoising layers?
  % compression?
  % ensembles
  % \item FGSM-generated samples
  \end{enumerate}

  \section{Results}

  \section{}

  \section{Lessons Learned}

  Selecting a bad learning rate can cause a model to fail to make full use of its capacity.
  Na\"{i}ve schemes like an exponentially decaying learning rate did not work particularly well because it's hard to tell \textit{a priori} what the base learning rate and the decay rate should be.
  % Adaptive schemes, such as the one-cycle learning rate policy, tended to work better because
\end{document}
